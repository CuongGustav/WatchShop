{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from requests_html import HTML\n",
    "from requests_html import HTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Page 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: Message: no such window: target window already closed\n",
      "from unknown error: web view not found\n",
      "  (Session info: chrome=130.0.6723.92)\n",
      "Stacktrace:\n",
      "\tGetHandleVerifier [0x00007FF7367E3AF5+28005]\n",
      "\t(No symbol) [0x00007FF7367483F0]\n",
      "\t(No symbol) [0x00007FF7365E580A]\n",
      "\t(No symbol) [0x00007FF7365BFA85]\n",
      "\t(No symbol) [0x00007FF736662AD7]\n",
      "\t(No symbol) [0x00007FF73667B1B1]\n",
      "\t(No symbol) [0x00007FF73665B7E3]\n",
      "\t(No symbol) [0x00007FF7366275C8]\n",
      "\t(No symbol) [0x00007FF736628731]\n",
      "\tGetHandleVerifier [0x00007FF736AD646D+3118813]\n",
      "\tGetHandleVerifier [0x00007FF736B26CC0+3448624]\n",
      "\tGetHandleVerifier [0x00007FF736B1CF3D+3408301]\n",
      "\tGetHandleVerifier [0x00007FF7368AA44B+841403]\n",
      "\t(No symbol) [0x00007FF73675344F]\n",
      "\t(No symbol) [0x00007FF73674F4C4]\n",
      "\t(No symbol) [0x00007FF73674F65D]\n",
      "\t(No symbol) [0x00007FF73673EBB9]\n",
      "\tBaseThreadInitThunk [0x00007FFE544F259D+29]\n",
      "\tRtlUserThreadStart [0x00007FFE5518AF38+40]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Your User Agent\")  # Add a user agent if needed\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "arr = []\n",
    "\n",
    "try:\n",
    "    for page in range(1, 2):  # Adjust the range as needed (e.g., 2 to 3)\n",
    "        link = f\"https://donghohaitrieu.com/danh-muc/dong-ho-nam/page/{page}\"\n",
    "        driver.get(link)\n",
    "\n",
    "        # Wait for the elements to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"woocommerce-LoopProduct-link\")))\n",
    "\n",
    "        # Find all <a> tags containing item links\n",
    "        a_tags = driver.find_elements(By.CLASS_NAME, \"woocommerce-LoopProduct-link\")\n",
    "\n",
    "        # Extract links\n",
    "        for a_tag in a_tags:\n",
    "            item_link = a_tag.get_attribute('href')\n",
    "            if item_link:  # Check if the link is not None\n",
    "                arr.append(item_link)\n",
    "\n",
    "        print(f\"Crawled page {page} successfully. Total links found: {len(arr)}\")\n",
    "\n",
    "    # Save links to CSV (append to existing file)\n",
    "    if arr:\n",
    "        df = pd.DataFrame(arr, columns=['links'])\n",
    "        df.to_csv('link_item.csv', mode='a', header=not pd.io.common.file_exists('link_item.csv'), index=False, encoding='utf-8')\n",
    "        print(\"Crawling complete. Links have been saved to link_item.csv.\")\n",
    "    else:\n",
    "        print(\"No links were collected.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Page 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled page 2 successfully. Total links found: 40\n",
      "Crawling complete. Links have been saved to link_item.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Your User Agent\")  # Add a user agent if needed\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "arr = []\n",
    "\n",
    "try:\n",
    "    for page in range(2, 3): \n",
    "        link = f\"https://donghohaitrieu.com/danh-muc/dong-ho-nam/page/{page}\"\n",
    "        driver.get(link)\n",
    "\n",
    "        # Wait for the elements to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"woocommerce-LoopProduct-link\")))\n",
    "\n",
    "        # Find all <a> tags containing item links\n",
    "        a_tags = driver.find_elements(By.CLASS_NAME, \"woocommerce-LoopProduct-link\")\n",
    "\n",
    "        # Extract links\n",
    "        for a_tag in a_tags:\n",
    "            item_link = a_tag.get_attribute('href')\n",
    "            if item_link:  # Check if the link is not None\n",
    "                arr.append(item_link)\n",
    "\n",
    "        print(f\"Crawled page {page} successfully. Total links found: {len(arr)}\")\n",
    "\n",
    "    # Save links to CSV (append to existing file)\n",
    "    if arr:\n",
    "        # Check if the file exists to determine if we need to write the header\n",
    "        file_exists = os.path.isfile('link_item.csv')\n",
    "        df = pd.DataFrame(arr, columns=['links'])\n",
    "        df.to_csv('link_item.csv', mode='a', header=not file_exists, index=False, encoding='utf-8')\n",
    "        print(\"Crawling complete. Links have been saved to link_item.csv.\")\n",
    "    else:\n",
    "        print(\"No links were collected.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Page 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawled page 3 successfully. Total links found: 40\n",
      "Crawling complete. Links have been saved to link_item.csv.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"user-agent=Your User Agent\")  # Add a user agent if needed\n",
    "driver = webdriver.Chrome(service=Service(), options=options)\n",
    "arr = []\n",
    "\n",
    "try:\n",
    "    for page in range(3, 4): \n",
    "        link = f\"https://donghohaitrieu.com/danh-muc/dong-ho-nam/page/{page}\"\n",
    "        driver.get(link)\n",
    "\n",
    "        # Wait for the elements to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, \"woocommerce-LoopProduct-link\")))\n",
    "\n",
    "        # Find all <a> tags containing item links\n",
    "        a_tags = driver.find_elements(By.CLASS_NAME, \"woocommerce-LoopProduct-link\")\n",
    "\n",
    "        # Extract links\n",
    "        for a_tag in a_tags:\n",
    "            item_link = a_tag.get_attribute('href')\n",
    "            if item_link:  # Check if the link is not None\n",
    "                arr.append(item_link)\n",
    "\n",
    "        print(f\"Crawled page {page} successfully. Total links found: {len(arr)}\")\n",
    "\n",
    "    # Save links to CSV (append to existing file)\n",
    "    if arr:\n",
    "        # Check if the file exists to determine if we need to write the header\n",
    "        file_exists = os.path.isfile('link_item.csv')\n",
    "        df = pd.DataFrame(arr, columns=['links'])\n",
    "        df.to_csv('link_item.csv', mode='a', header=not file_exists, index=False, encoding='utf-8')\n",
    "        print(\"Crawling complete. Links have been saved to link_item.csv.\")\n",
    "    else:\n",
    "        print(\"No links were collected.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawl Products Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>links</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://donghohaitrieu.com/san-pham/orient-cla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://donghohaitrieu.com/san-pham/orient-con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://donghohaitrieu.com/san-pham/orient-con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://donghohaitrieu.com/san-pham/orient-sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://donghohaitrieu.com/san-pham/orient-sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               links\n",
       "0  https://donghohaitrieu.com/san-pham/orient-cla...\n",
       "1  https://donghohaitrieu.com/san-pham/orient-con...\n",
       "2  https://donghohaitrieu.com/san-pham/orient-con...\n",
       "3  https://donghohaitrieu.com/san-pham/orient-sta...\n",
       "4  https://donghohaitrieu.com/san-pham/orient-sta..."
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.read_csv('.\\\\link_item.csv')\n",
    "frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def getdata(link):\n",
    "    # Khởi động Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "    \n",
    "    try:\n",
    "        # Chờ cho trang tải hoàn toàn trong 10 giây\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'thong-tin-san-pham')))\n",
    "        \n",
    "        # Lấy mã nguồn trang\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        \n",
    "        # Tạo từ điển để lưu dữ liệu\n",
    "        product_data = {}\n",
    "\n",
    "        # Tìm các thông tin cần thiết\n",
    "        info_tags = [\n",
    "            ('Thương hiệu', 'Thương hiệu:', 'a'),\n",
    "            ('Số hiệu sản phẩm', 'Số hiệu sản phẩm:', 'p'),\n",
    "            ('Bộ sưu tập', 'Bộ sưu tập:', 'a'),\n",
    "            ('Xuất xứ', 'Xuất xứ:', 'p'),\n",
    "            ('Giới tính', 'Giới tính:', 'p'),\n",
    "            ('Kính', 'Kính:', 'a'),\n",
    "            ('Máy', 'Máy:', 'a'),\n",
    "            ('Bảo hành quốc tế', 'Bảo hành quốc tế:', 'p'),\n",
    "            ('Bảo hành tại Hải Triều', 'Bảo hành tại Hải Triều:', 'p'),\n",
    "            ('Đường kính mặt số', 'Đường kính mặt số:', 'p'),\n",
    "            ('Bề dày mặt số', 'Bề dày mặt số:', 'p'),\n",
    "            ('Niềng', 'Niềng:', 'a'),\n",
    "            ('Dây đeo', 'Dây đeo:', 'p'),\n",
    "            ('Màu mặt số', 'Màu mặt số:', 'p'),\n",
    "            ('Chống nước', 'Chống nước:', 'p'),  # Cập nhật cho trường \"Chống nước\"\n",
    "            ('Chức năng', 'Chức năng:', 'p'),\n",
    "            ('Nơi sản xuất', 'Nơi sản xuất (Tùy từng lô hàng):', 'p'),  # Cập nhật cho trường \"Nơi sản xuất\"\n",
    "            ('Giá', 'price product-page-price', 'p'),\n",
    "        ]\n",
    "\n",
    "        for label, strong_text, tag in info_tags:\n",
    "            if label == 'Giá':\n",
    "                price_tag = soup.find('div', class_='product-price-container').find('p', class_='price product-page-price')\n",
    "                if price_tag:\n",
    "                    product_data[label] = price_tag.find('span', class_='woocommerce-Price-amount').text.strip()\n",
    "                else:\n",
    "                    product_data[label] = \"NaN\"\n",
    "            elif label == 'Chống nước':\n",
    "                water_resistance_tag = soup.find(string=lambda text: 'Chống nước:' in text)\n",
    "                if water_resistance_tag:\n",
    "                    water_resistance_paragraph = water_resistance_tag.find_parent('p')\n",
    "                    water_resistance_value = water_resistance_paragraph.get_text(strip=True).split(':')[-1].strip()\n",
    "                    product_data[label] = water_resistance_value\n",
    "                else:\n",
    "                    product_data[label] = \"NaN\"\n",
    "            elif label == 'Nơi sản xuất':\n",
    "                production_tag = soup.find(string=lambda text: 'Nơi sản xuất' in text)\n",
    "                if production_tag:\n",
    "                    production_paragraph = production_tag.find_parent('p')\n",
    "                    production_value = production_paragraph.get_text(strip=True).split(':')[-1].strip()\n",
    "                    product_data[label] = production_value\n",
    "                else:\n",
    "                    product_data[label] = \"NaN\"\n",
    "            else:\n",
    "                strong_tag = soup.find('strong', text=strong_text)\n",
    "                if strong_tag:\n",
    "                    if tag == 'a':\n",
    "                        product_data[label] = strong_tag.find_next(tag).text.strip()\n",
    "                    else:\n",
    "                        product_data[label] = strong_tag.next_sibling.strip() if strong_tag.next_sibling else \"NaN\"\n",
    "                else:\n",
    "                    product_data[label] = \"NaN\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {link}: {e}\")\n",
    "        product_data = {label: \"NaN\" for label, _, _ in info_tags}  # Ghi \"NaN\" nếu có lỗi\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return product_data  # Trả về từ điển chứa thông tin sản phẩm\n",
    "\n",
    "# Tạo danh sách để lưu dữ liệu\n",
    "all_products = []\n",
    "\n",
    "# Thực hiện trích xuất dữ liệu từ mỗi liên kết trong dataframe\n",
    "for index, link in enumerate(frame['links']):\n",
    "    product_info = getdata(link)\n",
    "    all_products.append(product_info)  # Thêm thông tin sản phẩm vào danh sách\n",
    "\n",
    "    # Chuyển danh sách thành DataFrame và lưu vào file CSV sau mỗi sản phẩm\n",
    "    df = pd.DataFrame(all_products)\n",
    "    df.dropna(inplace=True)\n",
    "    df.to_csv('.\\\\dataInfoCrawl.csv', encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "    print(f\"Processed {index + 1}/{len(frame['links'])}: {link}\")\n",
    "\n",
    "print(\"Data extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl Img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "def get_data(link):\n",
    "    # Khởi động Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "\n",
    "    product_data = {'Images': []}  # Khởi tạo từ điển để lưu dữ liệu\n",
    "\n",
    "    try:\n",
    "        # Chờ cho trang tải hoàn toàn\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'thong-tin-san-pham')))\n",
    "        \n",
    "        # Lấy mã nguồn trang\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Lấy ảnh chính\n",
    "        main_image_tag = soup.find('div', class_='flickity-slider').find('img')\n",
    "        if main_image_tag:\n",
    "            product_data['Images'].append(main_image_tag['src'])  # Thêm ảnh chính\n",
    "\n",
    "        # Lấy tất cả các ảnh phụ\n",
    "        for img_tag in soup.find_all('div', class_='col'):\n",
    "            img = img_tag.find('img')\n",
    "            if img and img['src'] != main_image_tag['src']:  # Đảm bảo không trùng với ảnh chính\n",
    "                product_data['Images'].append(img['src'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {link}: {e}\")\n",
    "        product_data['Images'] = [\"NaN\"]  # Ghi \"NaN\" nếu có lỗi\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return product_data  # Trả về từ điển chứa thông tin sản phẩm\n",
    "\n",
    "\n",
    "# Thực hiện trích xuất dữ liệu từ mỗi liên kết trong DataFrame\n",
    "all_products = [get_data(link) for link in frame['links']]\n",
    "\n",
    "# Chuyển danh sách thành DataFrame và lưu vào file CSV\n",
    "df = pd.DataFrame(all_products)\n",
    "df.dropna(inplace=True)  # Xóa các hàng có giá trị NaN\n",
    "df.to_csv('.\\\\dataImgCrawl.csv', encoding=\"utf-8-sig\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import os\n",
    "\n",
    "# Khởi tạo file CSV nếu chưa tồn tại\n",
    "file_path = './dataImgCrawl.csv'\n",
    "if not os.path.exists(file_path):\n",
    "    # Tạo file CSV với header khi lần đầu chạy\n",
    "    pd.DataFrame(columns=['Images']).to_csv(file_path, encoding=\"utf-8-sig\", index=False)\n",
    "\n",
    "def get_data(link):\n",
    "    # Khởi động Chrome WebDriver\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(link)\n",
    "\n",
    "    product_data = {'Images': []}  # Khởi tạo từ điển để lưu dữ liệu\n",
    "\n",
    "    try:\n",
    "        # Chờ cho trang tải hoàn toàn\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'thong-tin-san-pham')))\n",
    "        \n",
    "        # Lấy mã nguồn trang\n",
    "        page_source = driver.page_source\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Lấy ảnh chính\n",
    "        main_image_tag = soup.find('div', class_='flickity-slider').find('img')\n",
    "        if main_image_tag:\n",
    "            product_data['Images'].append(main_image_tag['src'])  # Thêm ảnh chính\n",
    "\n",
    "        # Lấy tất cả các ảnh phụ\n",
    "        for img_tag in soup.find_all('div', class_='col'):\n",
    "            img = img_tag.find('img')\n",
    "            if img and img['src'] != main_image_tag['src']:  # Đảm bảo không trùng với ảnh chính\n",
    "                product_data['Images'].append(img['src'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error accessing {link}: {e}\")\n",
    "        product_data['Images'] = [\"NaN\"]  # Ghi \"NaN\" nếu có lỗi\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return product_data  # Trả về từ điển chứa thông tin sản phẩm\n",
    "\n",
    "\n",
    "# Thực hiện trích xuất dữ liệu từ mỗi liên kết trong DataFrame\n",
    "for link in frame['links']:\n",
    "    product_info = get_data(link)\n",
    "\n",
    "    # Chuyển đổi product_info thành DataFrame để lưu vào file CSV\n",
    "    df = pd.DataFrame([product_info])\n",
    "    \n",
    "    # Lưu vào file CSV, không ghi header khi append\n",
    "    df.to_csv(file_path, mode='a', encoding=\"utf-8-sig\", index=False, header=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Đường dẫn tới file CSV đầu vào và đầu ra\n",
    "input_file = '.\\\\dataImgCrawl.csv'\n",
    "output_file = '.\\\\dataImgCrawl1.csv'\n",
    "\n",
    "# Đọc file CSV đầu vào và tạo file CSV mới\n",
    "with open(input_file, 'r', encoding='utf-8-sig') as file:\n",
    "    reader = csv.DictReader(file)\n",
    "    # Đặt tên cột cho file CSV đầu ra\n",
    "    with open(output_file, 'w', encoding='utf-8-sig', newline='') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        writer.writerow(['Image', 'OtherImage'])  # Tên các cột\n",
    "\n",
    "        for row in reader:\n",
    "            try:\n",
    "                # Lấy danh sách ảnh từ cột 'Images'\n",
    "                images = eval(row['Images'])\n",
    "                # Lọc các URL có định dạng ảnh\n",
    "                valid_images = [url for url in images if re.search(r'\\.(jpg|jpeg|png)$', url, re.IGNORECASE)]\n",
    "                \n",
    "                # Kiểm tra và ghi vào file mới\n",
    "                if valid_images:\n",
    "                    # Link ảnh đầu tiên vào cột 'Image'\n",
    "                    first_image = valid_images[0]\n",
    "                    # Các link ảnh còn lại vào cột 'OtherImage', nối thành một chuỗi\n",
    "                    other_images = ', '.join(valid_images[1:]) if len(valid_images) > 1 else ''\n",
    "                    writer.writerow([first_image, other_images])\n",
    "\n",
    "            except (KeyError, SyntaxError) as e:\n",
    "                print(f\"Lỗi khi xử lý dòng {row}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Đọc dữ liệu từ file đầu vào\n",
    "data_img = pd.read_csv('dataImgCrawl.csv')\n",
    "data_info = pd.read_csv('dataInfoCrawl.csv')\n",
    "\n",
    "# Ghép hai file theo chiều ngang (giữ nguyên thứ tự hàng)\n",
    "merged_data = pd.concat([data_img, data_info], axis=1)\n",
    "\n",
    "# Thêm cột ID chạy từ 1 đến số lượng hàng\n",
    "merged_data.insert(0, 'ID', range(1, len(merged_data) + 1))\n",
    "\n",
    "# Xuất dữ liệu ra file CSV mới\n",
    "merged_data.to_csv('dataCrawl.csv', index=False, encoding='utf-8-sig')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
